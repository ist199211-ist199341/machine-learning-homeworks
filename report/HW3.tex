\documentclass[12pt]{article}
\usepackage[paper=letterpaper,margin=2cm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{newtxtext, newtxmath}
\usepackage{enumitem}
\usepackage{titling}
\usepackage[colorlinks=true]{hyperref}
\usepackage{multirow}
\usepackage{svg}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{float}
\usepackage{graphicx}
\usepackage{subcaption}

\setlength{\droptitle}{-6em}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

% TEXT COLORS
% https://coolors.co/4c6085-1588e0-0cac8c-f25f5c-b89300
\definecolor{cgray}{HTML}{4c6085}
\definecolor{cblue}{HTML}{1588e0}
\definecolor{cgreen}{HTML}{0cac8c}
\definecolor{cred}{HTML}{f25f5c}
\definecolor{cyellow}{HTML}{b89300}

% BACKGROUND COLORS
% https://coolors.co/8a9cbc-7cc0f3-66f4d8-f6908e-ffe270
\definecolor{bgray}{HTML}{8a9cbc}
\definecolor{bblue}{HTML}{7cc0f3}
\definecolor{bgreen}{HTML}{66f4d8}
\definecolor{bred}{HTML}{f6908e}
\definecolor{byellow}{HTML}{ffe270}

\definecolor{linkcolor}{HTML}{f57429}
\definecolor{questioncolor}{HTML}{444444}

\hypersetup{
    allcolors=linkcolor
}

\lstdefinestyle{mystyle}{
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{style=mystyle}

\title{\large{Aprendizagem 2022}\vskip 0.2cm Homework III -- Group 020}
\date{}
\author{Diogo Correia (99211) \and Tom√°s Esteves (99341)}
\begin{document}
\maketitle
\begin{center}
    \large{\vskip -1.0cm\textbf{Part I}: Pen and paper}
\end{center}

{
\color{questioncolor}\bfseries
\noindent
Consider the problem of learning a regression model from 5 univariate observations\\
$\left((0.8), (1), (1.2), (1.4), (1.6)\right)$ with targets $\left(24, 20, 10, 13, 12\right)$.
}

\begin{enumerate}[leftmargin=\labelsep]
    \item {\color{questioncolor}\bfseries
          Consider the basis function, $\phi_j (x) = x^j$, for performing a 3-order
          polynomial regression,

          $$
              \hat{z}(x, w) = \sum_{j=0}^{3} w_j \phi_j (x)
              = w_0 + w_1 x + w_2 x^2 + w_3 x^3
          $$

          Learn the Ridge regression ($l_2$ regularization) on the transformed data
          space using the closed form solution with $\lambda = 2$.

          \textit{Hint:} use \texttt{numpy} matrix operations (e.g., \texttt{linalg.pinv} for inverse)
          to validate your calculus.
          }\\
          \vspace{0.5em}

          Considering the 5 observations, we can calculate the value of
          $\phi_0 (x)$, $\phi_1 (x)$, $\phi_2 (x)$ and $\phi_3 (x)$ for each
          of them.

          \begin{center}
              \captionsetup{type=table}
              \begin{tabular}{c|cccc}
                  $x$ & $\phi_0(x)$ & $\phi_1(x)$ & $\phi_2(x)$ & $\phi_3(x)$ \\
                  \hline
                  0.8 & 1           & 0.8         & 0.64        & 0.512       \\
                  1   & 1           & 1           & 1           & 1           \\
                  1.2 & 1           & 1.2         & 1.44        & 1.728       \\
                  1.4 & 1           & 1.4         & 1.96        & 2.744       \\
                  1.6 & 1           & 1.6         & 2.56        & 4.096
              \end{tabular}
              \captionof{table}{Value of $\phi_j(x)$ for each observation, $j=0\dots3$}
              \label{ex1-phi-table}
          \end{center}

          We shall consider $z$ as the vector of targets for the respective observations, $
              z = \begin{bmatrix}
                  24 & 20 & 10 & 13 & 12
              \end{bmatrix}^T
          $.

          Our goal is to perform a regression where the prediction is given by

          \begin{equation}\label{ex1-z-hat}
              \hat{z}(x, w)
              = \textcolor{cblue}{w_0} +
              \textcolor{cred}{w_1}x +
              \textcolor{cgreen}{w_2}x^2 +
              \textcolor{cyellow}{w_3}x^3
          \end{equation}

          We know the parameters in the Ridge regression are given by \eqref{ex1-ridge},
          where $X$ is the matrix after applying $\phi_j(x)$ to the observations,
          as per Table \ref{ex1-phi-table}.

          \begin{equation}\label{ex1-ridge}
              w = \left(X^T X + \lambda I\right)^{-1} \cdot X^T z
          \end{equation}

          $$
              X = \begin{bmatrix}{}
                  1.0000 & 0.8000 & 0.6400 & 0.5120 \\
                  1.0000 & 1.0000 & 1.0000 & 1.0000 \\
                  1.0000 & 1.2000 & 1.4400 & 1.7280 \\
                  1.0000 & 1.4000 & 1.9600 & 2.7440 \\
                  1.0000 & 1.6000 & 2.5600 & 4.0960
              \end{bmatrix}
              \quad
              \quad
              X^T =\begin{bmatrix}{}
                  1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 \\
                  0.8000 & 1.0000 & 1.2000 & 1.4000 & 1.6000 \\
                  0.6400 & 1.0000 & 1.4400 & 1.9600 & 2.5600 \\
                  0.5120 & 1.0000 & 1.7280 & 2.7440 & 4.0960
              \end{bmatrix}
          $$

          We can then calculate the remaining values, until we get the vector $w$.

          $$
              \begin{aligned}
                  X^T X                                    & = \begin{bmatrix}
                                                                   5.0000  & 6.0000  & 7.6000  & 10.0800 \\
                                                                   6.0000  & 7.6000  & 10.0800 & 13.8784 \\
                                                                   7.6000  & 10.0800 & 13.8784 & 19.6800 \\
                                                                   10.0800 & 13.8784 & 19.6800 & 28.5549
                                                               \end{bmatrix}                                                                                   \\
                  X^T X + \textcolor{cblue}{2} I           & = \begin{bmatrix}{}
                                                                   \colorbox{bblue}{$ 7.0000$} & 6.0000                      & 7.6000                       & 10.0800                      \\
                                                                   6.0000                      & \colorbox{bblue}{$ 9.6000$} & 10.0800                      & 13.8784                      \\
                                                                   7.6000                      & 10.0800                     & \colorbox{bblue}{$ 15.8784$} & 19.6800                      \\
                                                                   10.0800                     & 13.8784                     & 19.6800                      & \colorbox{bblue}{$ 30.5549$}
                                                               \end{bmatrix} \\
                  \left(X^T X + 2I\right) ^ {-1}           & = \begin{bmatrix}{}
                                                                   0.3417  & -0.1214 & -0.0749 & -0.0093 \\
                                                                   -0.1214 & 0.3892  & -0.0967 & -0.0745 \\
                                                                   -0.0749 & -0.0967 & 0.3726  & -0.1714 \\
                                                                   -0.0093 & -0.0745 & -0.1714 & 0.1800
                                                               \end{bmatrix}                                                                                   \\
                  \left(X^T X + 2I\right) ^ {-1} X^T       & = \begin{bmatrix}{}
                                                                   0.1918  & 0.1360  & 0.0720  & -0.0007 & -0.0825 \\
                                                                   0.0899  & 0.0966  & 0.0777  & 0.0297  & -0.0512 \\
                                                                   -0.0015 & 0.0296  & 0.0495  & 0.0498  & 0.0224  \\
                                                                   -0.0864 & -0.0751 & -0.0344 & 0.0445  & 0.1701
                                                               \end{bmatrix}                                                                         \\
                  w = \left(X^T X + 2I\right) ^ {-1} X^T z & = \begin{bmatrix}{}
                                                                   \textcolor{cblue}{7.0451} & \textcolor{cred}{4.6409} & \textcolor{cgreen}{1.9673} & \textcolor{cyellow}{-1.3009}
                                                               \end{bmatrix}^T
              \end{aligned}
          $$

          Having calculated the vector $w$, we now know the values of each
          weight in the regression.

          $$
              \begin{array}{cccc}
                  \textcolor{cblue}{w_0 = 7.0451}  &
                  \textcolor{cred}{w_1 = 4.6409}   &
                  \textcolor{cgreen}{w_2 = 1.9673} &
                  \textcolor{cyellow}{w_3 = -1.3009}
              \end{array}
          $$

          Replacing them in \eqref{ex1-z-hat} gives us the regression expression,

          \begin{equation}\label{ex1-final-z-hat}
              \hat{z}(x)
              = \textcolor{cblue}{7.0451} +
              \textcolor{cred}{4.6409}x +
              \textcolor{cgreen}{1.9673}x^2
              \textcolor{cyellow}{-1.3009}x^3
          \end{equation}

    \item {\color{questioncolor}\bfseries
          Compute the training RMSE for the learnt regression model.
          }\\
          \vspace{0.5em}

          The RMSE (Root Mean Square Error) is given by

          \begin{equation}\label{ex2-rmse}
              E(w) = \sqrt{\frac{1}{N} \sum_{i = 1}^{N} (z_i - \hat{z}_i)^2}
          \end{equation}

          Since we have 5 observations, $N = 5$.

          Using \eqref{ex1-final-z-hat} from the previous exercise, we can determine
          $\hat{z}$ for the 5 observations.
          Using matrix multiplication:

          $$
              \begin{aligned}
                  X            & = \begin{bmatrix}{}
                                       1.0000 & 0.8000 & 0.6400 & 0.5120 \\
                                       1.0000 & 1.0000 & 1.0000 & 1.0000 \\
                                       1.0000 & 1.2000 & 1.4400 & 1.7280 \\
                                       1.0000 & 1.4000 & 1.9600 & 2.7440 \\
                                       1.0000 & 1.6000 & 2.5600 & 4.0960
                                   \end{bmatrix} &
                  w            & = \begin{bmatrix}{}
                                       \textcolor{cblue}{7.0451}  \\
                                       \textcolor{cred}{4.6409}   \\
                                       \textcolor{cgreen}{1.9673} \\
                                       \textcolor{cyellow}{-1.3009}
                                   \end{bmatrix}       \\
                  \hat{z} = Xw & = \begin{bmatrix}{}
                                       11.3509 \\
                                       12.3525 \\
                                       13.1992 \\
                                       13.8287 \\
                                       14.1785
                                   \end{bmatrix}
                               &
                  z            & = \begin{bmatrix}
                                       24 \\ 20 \\ 10 \\ 13 \\ 12
                                   \end{bmatrix}
              \end{aligned}
          $$

          We can now calculate RMSE, using \eqref{ex2-rmse},

          $$
              \begin{aligned}
                  \sum_{i = 1}^{N} (z_i - \hat{z}_i)^2 & = 234.153                           \\
                  E(w)                                 & = \sqrt{\frac{1}{5} \times 234.153} \\
                                                       & = 6.843
              \end{aligned}
          $$

          Therefore, the RMSE is \textbf{6.843}.

    \item {\color{questioncolor}\bfseries
          Consider a multi-layer perceptron characterized by one hidden layer with 2 nodes.
          Using the activation function $f(x) = e^{0.1 x}$ on all units, all weights
          initialized as 1 (including biases), and the half squared error loss,
          perform one batch gradient descent update (with learning rate $\eta = 0.1$)
          for the first three observations $(0.8)$, $(1)$ and $(1.2)$.
          }\\
          \vspace{0.5em}

          // TODO
\end{enumerate}

\pagebreak

\begin{center}
    \large{\textbf{Part II}: Programming and critical analysis}
\end{center}

{\color{questioncolor}\bfseries
\noindent
Consider the following three regressors applied on \texttt{kin8nm.arff} data
(available at the webpage):
\begin{itemize}
    \item linear regression with Ridge regularization term of 0.1
    \item two MLPs - $MLP_1$ and $MLP_2$ - each with two hidden layers of
          size 10, hyperbolic tagent function as the activation function of all nodes,
          a maximum of 500 iterations, and a fixed seed (\texttt{random\_state=0}).
          $MLP_1$ should be parameterized with early stopping while $MLP_2$ should not
          consider early stopping.
          Remaining parameters (e.g., loss function, batch size, regularization term,
          solver) should be set as default.
\end{itemize}
Using a 70-30 training-test split with a fixed seed (\texttt{random\_state=0}):
}

\begin{enumerate}[leftmargin=\labelsep,resume]
    \item {\color{questioncolor}\bfseries
          Compute the MAE of the three regressors: linear regression, $MLP_1$ and $MLP_2$.
          }\\
          \vspace{0.5em}

          // TODO

    \item {\color{questioncolor}\bfseries
          Plot the residues (in absolute value) using two visualizations: boxplots
          and histograms.\\
          \textit{Hint}: consider using \texttt{boxplot} and \texttt{hist} functions
          from \texttt{matplotlib.pyplot} to this end.
          }\\
          \vspace{0.5em}

          // TODO
    \item {\color{questioncolor}\bfseries
          How many iterations were required for $MLP_1$ and $MLP_2$ to converge?
          }\\
          \vspace{0.5em}

          // TODO
    \item {\color{questioncolor}\bfseries
          What can be motivating the unexpected differences on the number of iterations?
          Hypothesize one reason underlying the observed performance differences between the MLPs.
          }\\
          \vspace{0.5em}

          // TODO

\end{enumerate}

\pagebreak

\center\large{\textbf{Appendix}\vskip 0.3cm}

// TODO

\end{document}

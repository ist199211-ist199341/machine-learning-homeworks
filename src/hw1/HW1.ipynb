{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework I\n",
    "\n",
    "Diogo Correia (ist199211) & Tom√°s Esteves (ist199341)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Pen and Paper [12v]\n",
    "\n",
    "**Given the following decision tree learnt from 20 observation using Shannon entropy, with leaf annotations (`#correct/#total`)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Decision Tree](./decision_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) [4v] Draw the training confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <td colspan=\"2\" rowspan=\"2\" style=\"border-top: none; border-left: none;\"></td>\n",
    "    <th colspan=\"2\">True</th>\n",
    "    <td rowspan=\"2\" style=\"border-top: none; border-right: none;\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Positive</th>\n",
    "    <th>Negative</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th rowspan=\"2\">Predicted</th>\n",
    "    <th>Positive</th>\n",
    "    <td>8</td>\n",
    "    <td>4</td>\n",
    "    <td>12</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Negative</th>\n",
    "    <td>3</td>\n",
    "    <td>5</td>\n",
    "    <td>8</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th colspan=\"2\" style=\"border-left: none; border-bottom: none;\"></th>\n",
    "    <td>11</td>\n",
    "    <td>9</td>\n",
    "    <td>20</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) [3v] Identify the training F1 after a post-pruning of the given tree under a maximum depth of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <td colspan=\"2\" rowspan=\"2\" style=\"border-top: none; border-left: none;\"></td>\n",
    "    <th colspan=\"2\">True</th>\n",
    "    <td rowspan=\"2\" style=\"border-top: none; border-right: none;\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Positive</th>\n",
    "    <th>Negative</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th rowspan=\"2\">Predicted</th>\n",
    "    <th>Positive</th>\n",
    "    <td>5</td>\n",
    "    <td>2</td>\n",
    "    <td>7</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Negative</th>\n",
    "    <td>6</td>\n",
    "    <td>7</td>\n",
    "    <td>13</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th colspan=\"2\" style=\"border-left: none; border-bottom: none;\"></th>\n",
    "    <td>11</td>\n",
    "    <td>9</td>\n",
    "    <td>20</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positives = 5\n",
    "false_positives = 2\n",
    "false_negatives = 6\n",
    "\n",
    "precision = true_positives / (true_positives + false_positives)\n",
    "recall = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "f1_measure = (0.5 * (1 / precision + 1 / recall)) ** (-1)\n",
    "\n",
    "f1_measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) [2v] Identify two different reasons as to why the left tree path was not further decomposed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left tree path might not have been further decomposed because:\n",
    "\n",
    "- We did not want to overfit the model, since we have a very small sample size.\n",
    "  For this reason, if we were to further decompose the left tree path, we might end up with a less accurate\n",
    "  decision tree, since the 2 negative observations might have been outliers.\n",
    "- The information gain of this branch, $IG(y_{out} | y_2, y_1 = A)$, might be very small,\n",
    "  since there are a lot more observations classified as positive than as negative.\n",
    "  If we were to decompose the left path, there might be no optimal division that would correctly identify all observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) [3v] Compute the information gain of variable y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log2\n",
    "import operator as op\n",
    "from itertools import chain\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT\n",
    "total_positive_count = 11\n",
    "total_negative_count = 9\n",
    "\n",
    "branch_a_positive_count = 5\n",
    "branch_a_negative_count = 2\n",
    "\n",
    "branch_b_positive_count = 6\n",
    "branch_b_negative_count = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def entropy_by_count(counts):\n",
    "    \"\"\"\n",
    "    Calculates the information entropy, I(X), of a set, given the count of each class of element\n",
    "    \"\"\"\n",
    "    total = sum(counts)\n",
    "    return reduce(op.add, map(lambda x: -(x / total) * log2(x / total), counts))\n",
    "\n",
    "\n",
    "def split_entropy_by_count(branch_counts):\n",
    "    \"\"\"\n",
    "    Calculates the entropy after branching on a variable\n",
    "    \"\"\"\n",
    "    # branch counts is a list of int lists\n",
    "    total = sum(chain(*branch_counts))\n",
    "    return reduce(\n",
    "        op.add, map(lambda x: (sum(x) / total) * entropy_by_count(x), branch_counts)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_y_out = entropy_by_count([total_positive_count, total_negative_count])\n",
    "entropy_y_out_y1 = split_entropy_by_count(\n",
    "    [\n",
    "        [branch_a_positive_count, branch_a_negative_count],\n",
    "        [branch_b_positive_count, branch_b_negative_count],\n",
    "    ]\n",
    ")\n",
    "\n",
    "information_gain = entropy_y_out - entropy_y_out_y1\n",
    "\n",
    "information_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming [8v]\n",
    "\n",
    "**Considering the `pd_speech.arff` dataset available at the homework tab:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) [6v]\n",
    "\n",
    "**Using sklearn, apply a stratified 70-30 training-testing split with a fixed seed\n",
    "(`random_state=1`), and assess in a single plot the training and testing accuracies of a decision tree\n",
    "with no depth limits (and remaining default behavior) for a varying number of selected features\n",
    "in `{5,10,40,100,250,700}`. Feature selection should be performed before decision tree learning\n",
    "considering the discriminative power of the input variables according to mutual information\n",
    "criterion (`mutual_info_classif`).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import pandas as pd\n",
    "from scipy.io.arff import loadarff\n",
    "from sklearn import feature_selection, model_selection, tree, metrics, preprocessing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the ARFF file\n",
    "data = loadarff(\"data/pd_speech.arff\")\n",
    "df = pd.DataFrame(data[0])\n",
    "df[\"class\"] = df[\"class\"].str.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features from the outcome (class)\n",
    "X = df.drop(\"class\", axis=1)\n",
    "y = df[\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into a training set (70%) and a testing set (30%)\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X, y, train_size=0.7, stratify=y, random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FEATURES = [5, 10, 40, 100, 250, 700]\n",
    "\n",
    "training_accurancy = []\n",
    "test_accurancy = []\n",
    "\n",
    "for num_features in NUM_FEATURES:\n",
    "    # Select the features with the greatest information gain (by mutual_info_classif)\n",
    "    kbest = feature_selection.SelectKBest(\n",
    "        score_func=lambda a, b: feature_selection.mutual_info_classif(\n",
    "            a, b, random_state=1\n",
    "        ),\n",
    "        k=num_features,\n",
    "    )\n",
    "    kbest.fit(X_train, y_train)\n",
    "\n",
    "    # Get the observation sets with only the selected N best features\n",
    "    X_train_cut = kbest.transform(X_train)\n",
    "    X_test_cut = kbest.transform(X_test)\n",
    "\n",
    "    # Fit the decision tree classifier\n",
    "    predictor = tree.DecisionTreeClassifier(random_state=1)\n",
    "    predictor.fit(X_train_cut, y_train)\n",
    "\n",
    "    # Use the decision tree to predict the outcome of the given observations\n",
    "    y_train_pred = predictor.predict(X_train_cut)\n",
    "    y_test_pred = predictor.predict(X_test_cut)\n",
    "\n",
    "    # Get the accuracy of each test\n",
    "    train_acc = metrics.accuracy_score(y_train, y_train_pred)\n",
    "    test_acc = metrics.accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "    training_accurancy.append(train_acc)\n",
    "    test_accurancy.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "    NUM_FEATURES,\n",
    "    training_accurancy,\n",
    "    label=\"Training Accuraccy\",\n",
    "    marker=\"+\",\n",
    "    color=\"#4caf50\",\n",
    ")\n",
    "plt.plot(\n",
    "    NUM_FEATURES, test_accurancy, label=\"Test Accuraccy\", marker=\".\", color=\"#ff5722\"\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Number of Selected Features\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(\"../../report/assets/hw1-plot.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) [2v]\n",
    "\n",
    "**Why training accuracy is persistently 1? Critically analyze the gathered results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the obtained results, we noticed that the training accuracy is always 1, regardless of the number of selected features.\n",
    "This is a result of how decision trees learn.\n",
    "\n",
    "Since the question prompt tells us the decision tree does not have a depth limit, a decision tree that perfectly fits all the training data (`X_train`) can be created.\n",
    "Therefore, after the tree is trained, if we give the training set (`X_train`) as the data set to test its accuracy, it'll know the correct path for all of the observations and knows how to classify them.\n",
    "This results in an accuracy of 1.\n",
    "\n",
    "However, if we test the model with a data set that it hasn't been trained on (`X_test`), we see its accuracy slightly decreases to around 0.8.\n",
    "This happens because it has never seen those observations before, so it might have leaves that are not expanded enough to accurately classify them.\n",
    "\n",
    "Furthermore, we can also notice that the accuracy of the decision tree changes with the number of features.\n",
    "Using a lot of features to train the model can produce an overfitted tree, reducing its accuracy when predicting the outcome of a new observation,\n",
    "while using only a few features might not be enough information to train the decision tree.\n",
    "It's important to find the right features to include in the decision tree, and we can see that both `N = 40` and `N = 250` are good candidates for the number of features to improve, although not by much."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}

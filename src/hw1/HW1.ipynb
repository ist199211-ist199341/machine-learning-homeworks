{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework I\n",
    "\n",
    "Diogo Correia (ist199211) & Tomás Esteves (ist199341)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Pen and Paper [12v]\n",
    "\n",
    "**Given the following decision tree learnt from 20 observation using Shannon entropy, with leaf annotations (`#correct/#total`)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Decision Tree](./decision_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) [4v] Draw the training confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <td colspan=\"2\" rowspan=\"2\" style=\"border-top: none; border-left: none;\"></td>\n",
    "    <th colspan=\"2\">True</th>\n",
    "    <td rowspan=\"2\" style=\"border-top: none; border-right: none;\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Positive</th>\n",
    "    <th>Negative</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th rowspan=\"2\">Predicted</th>\n",
    "    <th>Positive</th>\n",
    "    <td>8</td>\n",
    "    <td>4</td>\n",
    "    <td>12</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Negative</th>\n",
    "    <td>3</td>\n",
    "    <td>5</td>\n",
    "    <td>8</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th colspan=\"2\" style=\"border-left: none; border-bottom: none;\"></th>\n",
    "    <td>11</td>\n",
    "    <td>9</td>\n",
    "    <td>13</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) [3v] Identify the training F1 after a post-pruning of the given tree under a maximum depth of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <td colspan=\"2\" rowspan=\"2\" style=\"border-top: none; border-left: none;\"></td>\n",
    "    <th colspan=\"2\">True</th>\n",
    "    <td rowspan=\"2\" style=\"border-top: none; border-right: none;\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Positive</th>\n",
    "    <th>Negative</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th rowspan=\"2\">Predicted</th>\n",
    "    <th>Positive</th>\n",
    "    <td>5</td>\n",
    "    <td>2</td>\n",
    "    <td>7</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Negative</th>\n",
    "    <td>6</td>\n",
    "    <td>7</td>\n",
    "    <td>13</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th colspan=\"2\" style=\"border-left: none; border-bottom: none;\"></th>\n",
    "    <td>11</td>\n",
    "    <td>9</td>\n",
    "    <td>12</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = 5\n",
    "FP = 2\n",
    "FN = 6\n",
    "\n",
    "P = TP / (TP + FP)\n",
    "R = TP / (TP + FN)\n",
    "\n",
    "F = (1 / 2 * (1 / P + 1 / R)) ** (-1)\n",
    "\n",
    "F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) [2v] Identify two different reasons as to why the left tree path was not further decomposed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) To avoid overfitting (?)\n",
    "2) Se A tiver correspondência direta em ser positivo (?)\n",
    "3) IG desse branch ser muito pequeno (?)\n",
    "4) Número de folhas limitado a 3 (?)\n",
    "5) How can you correct bias towards the dominant class? To mitigate decision trees’ bias towards predicting the dominant class, make sure to adjust class imbalance before fitting your model. There are three approaches for tackling class imbalance in the preprocessing stage (or data cleaning stage) https://www.keboola.com/blog/decision-trees-machine-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) [3v] Compute the information gain of variable y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log2\n",
    "\n",
    "pos_ratio = 11 / 20\n",
    "\n",
    "neg_ratio = 9 / 20\n",
    "\n",
    "A_ratio = 7 / 20\n",
    "\n",
    "B_ratio = 13 / 20\n",
    "\n",
    "A_Positive_Ratio = 5 / 7\n",
    "\n",
    "A_Negative_Ratio = 2 / 7\n",
    "\n",
    "B_Positive_Ratio = 6 / 13\n",
    "\n",
    "B_Negative_Ratio = 7 / 13\n",
    "\n",
    "E_y_out = - pos_ratio * log2(pos_ratio) - neg_ratio * log2(neg_ratio)\n",
    "\n",
    "E_y_out_y1 = A_ratio * (\n",
    "    - A_Positive_Ratio * log2(A_Positive_Ratio)\n",
    "    - A_Negative_Ratio * log2(A_Negative_Ratio)\n",
    ") + B_ratio * (\n",
    "    - B_Positive_Ratio * log2(B_Positive_Ratio)\n",
    "    - B_Negative_Ratio * log2(B_Negative_Ratio)\n",
    ")\n",
    "\n",
    "IG = E_y_out - E_y_out_y1\n",
    "\n",
    "IG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming [8v]\n",
    "\n",
    "**Considering the `pd_speech.arff` dataset available at the homework tab:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) [6v]\n",
    "\n",
    "**Using sklearn, apply a stratified 70-30 training-testing split with a fixed seed\n",
    "(`random_state=1`), and assess in a single plot the training and testing accuracies of a decision tree\n",
    "with no depth limits (and remaining default behavior) for a varying number of selected features\n",
    "in `{5,10,40,100,250,700}`. Feature selection should be performed before decision tree learning\n",
    "considering the discriminative power of the input variables according to mutual information\n",
    "criterion (mutual_info_classif).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import pandas as pd\n",
    "from scipy.io.arff import loadarff\n",
    "from sklearn import feature_selection, model_selection, tree, metrics, preprocessing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the ARFF file\n",
    "data = loadarff(\"data/pd_speech.arff\")\n",
    "df = pd.DataFrame(data[0])\n",
    "df[\"class\"] = df[\"class\"].str.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"class\", axis=1)\n",
    "y = df[\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutualInfoClassif = feature_selection.mutual_info_classif(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = {}\n",
    "\n",
    "for i in range(0, len(mutualInfoClassif)):\n",
    "    values[df.columns[i]] = mutualInfoClassif[i]\n",
    "\n",
    "# FIXME (why? idk)\n",
    "values[\"0\"] = 1\n",
    "values[\"1\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X, y, train_size=0.7, stratify=y, random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FEATURES = [5, 10, 40, 100, 250, 700]\n",
    "\n",
    "training_accurancy = []\n",
    "test_accurancy = []\n",
    "\n",
    "for num_features in NUM_FEATURES:\n",
    "    values_sorted = {}\n",
    "    a = num_features\n",
    "\n",
    "    for column, info in sorted(values.items(), key=itemgetter(1), reverse=True):\n",
    "        if a > 0:\n",
    "            values_sorted[column] = info\n",
    "        else:\n",
    "            values_sorted[column] = 0\n",
    "        a -= 1\n",
    "\n",
    "    predictor = tree.DecisionTreeClassifier(class_weight=values_sorted)\n",
    "\n",
    "    predictor.fit(X_train, y_train)\n",
    "\n",
    "    y_train_pred = predictor.predict(X_train)\n",
    "    y_test_pred = predictor.predict(X_test)\n",
    "\n",
    "    train_acc = metrics.accuracy_score(y_train, y_train_pred)\n",
    "    test_acc = metrics.accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "    training_accurancy.append(train_acc)\n",
    "    test_accurancy.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(NUM_FEATURES, training_accurancy, label=\"Training Accuraccy\")\n",
    "plt.plot(NUM_FEATURES, test_accurancy, label=\"Test Accuraccy\")\n",
    "\n",
    "plt.xlabel(\"Number of selected features\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) [2v]\n",
    "\n",
    "**Why training accuracy is persistently 1? Critically analyze the gathered results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pelos resultados observados podemos notar que a training accuraccy é de 1.\n",
    "Isto deve-se à forma de como as decisions trees aprendem.\n",
    "Dado que o enunciado indica que a a árvore não tem limite de profundidade, para cada dado de treino inserido será criado uma nova folha na árvore, que tem como \"caminho\" os seus valores de input.\n",
    "Assim depois da árvore estar treinada, se lhe dermos os mesmos inputs (de treino) para testar a sua accuraccy, esta já sabe o \"caminho\" a fazer e sabe classificar corretamente os valores. Isto leva a que tenha uma accuraccy de 1. No entanto ao ser lhe dada uma amostra para qual não foi treinada (de teste), observa-se que a accuraccy diminui para os 0.8. Isto porque não tem os tais caminhos criados que lhe permitem saber exatamente o valor."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

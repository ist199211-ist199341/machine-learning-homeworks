{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework I\n",
    "\n",
    "Diogo Correia (ist199211) & Tomás Esteves (ist199341)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Pen and Paper [12v]\n",
    "\n",
    "**Given the following decision tree learnt from 20 observation using Shannon entropy, with leaf annotations (`#correct/#total`)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Decision Tree](./decision_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) [4v] Draw the training confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <td colspan=\"2\" rowspan=\"2\" style=\"border-top: none; border-left: none;\"></td>\n",
    "    <th colspan=\"2\">True</th>\n",
    "    <td rowspan=\"2\" style=\"border-top: none; border-right: none;\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Positive</th>\n",
    "    <th>Negative</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th rowspan=\"2\">Predicted</th>\n",
    "    <th>Positive</th>\n",
    "    <td>8</td>\n",
    "    <td>4</td>\n",
    "    <td>12</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Negative</th>\n",
    "    <td>3</td>\n",
    "    <td>5</td>\n",
    "    <td>8</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th colspan=\"2\" style=\"border-left: none; border-bottom: none;\"></th>\n",
    "    <td>11</td>\n",
    "    <td>9</td>\n",
    "    <td>20</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) [3v] Identify the training F1 after a post-pruning of the given tree under a maximum depth of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <td colspan=\"2\" rowspan=\"2\" style=\"border-top: none; border-left: none;\"></td>\n",
    "    <th colspan=\"2\">True</th>\n",
    "    <td rowspan=\"2\" style=\"border-top: none; border-right: none;\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Positive</th>\n",
    "    <th>Negative</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th rowspan=\"2\">Predicted</th>\n",
    "    <th>Positive</th>\n",
    "    <td>5</td>\n",
    "    <td>2</td>\n",
    "    <td>7</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Negative</th>\n",
    "    <td>6</td>\n",
    "    <td>7</td>\n",
    "    <td>13</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th colspan=\"2\" style=\"border-left: none; border-bottom: none;\"></th>\n",
    "    <td>11</td>\n",
    "    <td>9</td>\n",
    "    <td>20</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positives = 5\n",
    "false_positives = 2\n",
    "false_negatives = 6\n",
    "\n",
    "precision = true_positives / (true_positives + false_positives)\n",
    "recall = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "f1_measure = (0.5 * (1 / precision + 1 / recall)) ** (-1)\n",
    "\n",
    "f1_measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) [2v] Identify two different reasons as to why the left tree path was not further decomposed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left tree path might not have been further decomposed because:\n",
    "\n",
    "- We did not want to overfit the model, since we have a very small sample size.\n",
    "- The information gain of this branch is very small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) To avoid overfitting (?)\n",
    "2) Se A tiver correspondência direta em ser positivo (?)\n",
    "3) IG desse branch ser muito pequeno (?)\n",
    "4) Número de folhas limitado a 3 (?)\n",
    "5) How can you correct bias towards the dominant class? To mitigate decision trees’ bias towards predicting the dominant class, make sure to adjust class imbalance before fitting your model. There are three approaches for tackling class imbalance in the preprocessing stage (or data cleaning stage) https://www.keboola.com/blog/decision-trees-machine-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) [3v] Compute the information gain of variable y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log2\n",
    "\n",
    "pos_ratio = 11 / 20\n",
    "\n",
    "neg_ratio = 9 / 20\n",
    "\n",
    "A_ratio = 7 / 20\n",
    "\n",
    "B_ratio = 13 / 20\n",
    "\n",
    "A_Positive_Ratio = 5 / 7\n",
    "\n",
    "A_Negative_Ratio = 2 / 7\n",
    "\n",
    "B_Positive_Ratio = 6 / 13\n",
    "\n",
    "B_Negative_Ratio = 7 / 13\n",
    "\n",
    "E_y_out = -pos_ratio * log2(pos_ratio) - neg_ratio * log2(neg_ratio)\n",
    "\n",
    "E_y_out_y1 = A_ratio * (\n",
    "    -A_Positive_Ratio * log2(A_Positive_Ratio)\n",
    "    - A_Negative_Ratio * log2(A_Negative_Ratio)\n",
    ") + B_ratio * (\n",
    "    -B_Positive_Ratio * log2(B_Positive_Ratio)\n",
    "    - B_Negative_Ratio * log2(B_Negative_Ratio)\n",
    ")\n",
    "\n",
    "IG = E_y_out - E_y_out_y1\n",
    "\n",
    "IG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming [8v]\n",
    "\n",
    "**Considering the `pd_speech.arff` dataset available at the homework tab:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) [6v]\n",
    "\n",
    "**Using sklearn, apply a stratified 70-30 training-testing split with a fixed seed\n",
    "(`random_state=1`), and assess in a single plot the training and testing accuracies of a decision tree\n",
    "with no depth limits (and remaining default behavior) for a varying number of selected features\n",
    "in `{5,10,40,100,250,700}`. Feature selection should be performed before decision tree learning\n",
    "considering the discriminative power of the input variables according to mutual information\n",
    "criterion (mutual_info_classif).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import pandas as pd\n",
    "from scipy.io.arff import loadarff\n",
    "from sklearn import feature_selection, model_selection, tree, metrics, preprocessing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the ARFF file\n",
    "data = loadarff(\"data/pd_speech.arff\")\n",
    "df = pd.DataFrame(data[0])\n",
    "df[\"class\"] = df[\"class\"].str.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"class\", axis=1)\n",
    "y = df[\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutualInfoClassif = feature_selection.mutual_info_classif(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ig = pd.DataFrame(mutualInfoClassif, columns=[\"values\"])\n",
    "df_ig.index = X.columns\n",
    "df_ig = df_ig.sort_values(by=[\"values\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X, y, train_size=0.7, stratify=y, random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FEATURES = [5, 10, 40, 100, 250, 700]\n",
    "\n",
    "training_accurancy = []\n",
    "test_accurancy = []\n",
    "\n",
    "for num_features in NUM_FEATURES:\n",
    "\n",
    "    df_ig_cut = df_ig[:num_features]\n",
    "    X_train_cut = X_train[df_ig_cut.index]\n",
    "    X_test_cut = X_test[df_ig_cut.index]\n",
    "\n",
    "    predictor = tree.DecisionTreeClassifier()\n",
    "    predictor.fit(X_train_cut, y_train)\n",
    "\n",
    "    y_train_pred = predictor.predict(X_train_cut)\n",
    "    y_test_pred = predictor.predict(X_test_cut)\n",
    "\n",
    "    train_acc = metrics.accuracy_score(y_train, y_train_pred)\n",
    "    test_acc = metrics.accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "    training_accurancy.append(train_acc)\n",
    "    test_accurancy.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(NUM_FEATURES, training_accurancy, label=\"Training Accuraccy\")\n",
    "plt.plot(NUM_FEATURES, test_accurancy, label=\"Test Accuraccy\")\n",
    "\n",
    "plt.xlabel(\"Number of Selected Features\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) [2v]\n",
    "\n",
    "**Why training accuracy is persistently 1? Critically analyze the gathered results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the obtained results, we noticed that the training accuracy is always 1, regardless of the number of selected features.\n",
    "This is a result of how the decision trees learn.  \n",
    "\n",
    "Since the question prompt tells us the tree does not have a depth limit, for each element in the training set, a new leaf will be created, which has its values as the path on the tree.\n",
    "Therefore, after the tree is trained, if we give the training set as the data set to test its accuracy, it'll know the correct path for all of the elements and knows how to classify them.\n",
    "This results in an accuracy of 1.\n",
    "\n",
    "However, if we test the model with a data set it hasn't been trained on, we see it's accuracy slightly decreases to around 0.8.\n",
    "This happens because it has never seen the values before, so it might have leaves that are not expanded enough to accurately classify the elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pelos resultados observados podemos notar que a training accuraccy é de 1.\n",
    "Isto deve-se à forma de como as decisions trees aprendem.\n",
    "Dado que o enunciado indica que a a árvore não tem limite de profundidade, para cada dado de treino inserido será criado uma nova folha na árvore, que tem como \"caminho\" os seus valores de input.\n",
    "Assim depois da árvore estar treinada, se lhe dermos os mesmos inputs (de treino) para testar a sua accuraccy, esta já sabe o \"caminho\" a fazer e sabe classificar corretamente os valores. Isto leva a que tenha uma accuraccy de 1. No entanto ao ser lhe dada uma amostra para qual não foi treinada (de teste), observa-se que a accuraccy diminui para os 0.8. Isto porque não tem os tais caminhos criados que lhe permitem saber exatamente o valor."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pen and Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given the following decision tree learnt from 20 observation using Shannon entropy, with leaf annotations (#correct/#total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Decision Tree](./decision_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Draw the training confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{matrix}\n",
    " &  &  & True &  & \\\\\n",
    " &  & Positive &  & Negative & \\\\\n",
    " & Positive & 8 &  & 4 & 12\\\\\n",
    "Predicted &  &  &  &  & \\\\\n",
    " & Negative & 3 &  & 5 & 8\\\\\n",
    " &  &  &  &  & \\\\\n",
    " &  & 11 &  & 9 & 13\n",
    "\\end{matrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Identify the training F1 after a post-pruning of the given tree under a maximum depth of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{matrix}\n",
    " &  &  & True &  & \\\\\n",
    " &  & Positive &  & Negative & \\\\\n",
    " & Positive & 5 &  & 2 & 7\\\\\n",
    "Predicted &  &  &  &  & \\\\\n",
    " & Negative & 6 &  & 7 & 13\\\\\n",
    " &  &  &  &  & \\\\\n",
    " &  & 11 &  & 9 & 12\n",
    "\\end{matrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 5 / 7\n",
    "R = 5 / 11\n",
    "\n",
    "F = (1 / 2 * (1 / P + 1 / R)) ** (-1)\n",
    "\n",
    "F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Identify two different reasons as to why the left tree path was not further decomposed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Compute the information gain of variable y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log2\n",
    "\n",
    "pos_ratio = 11 / 20\n",
    "\n",
    "neg_ratio = 9 / 20\n",
    "\n",
    "A_ratio = 7 / 20\n",
    "\n",
    "B_ratio = 13 / 20\n",
    "\n",
    "A_Positive_Ratio = 5 / 7\n",
    "\n",
    "A_Negative_Ratio = 2 / 7\n",
    "\n",
    "B_Positive_Ratio = 6 / 13\n",
    "\n",
    "B_Negative_Ratio = 7 / 13\n",
    "\n",
    "E_y_out = -pos_ratio * log2(pos_ratio) - neg_ratio * log2(neg_ratio)\n",
    "\n",
    "E_y_out_y1 = A_ratio * (\n",
    "    -A_Positive_Ratio * log2(A_Positive_Ratio)\n",
    "    - A_Negative_Ratio * log2(A_Negative_Ratio)\n",
    ") + B_ratio * (\n",
    "    -B_Positive_Ratio * log2(B_Positive_Ratio)\n",
    "    - B_Negative_Ratio * log2(B_Negative_Ratio)\n",
    ")\n",
    "\n",
    "IG = E_y_out - E_y_out_y1\n",
    "\n",
    "IG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Considering the pd_speech.arff dataset available at the homework tab:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1\n",
    "\n",
    "Using sklearn, apply a stratified 70-30 training-testing split with a fixed seed\n",
    "(random_state=1), and assess in a single plot the training and testing accuracies of a decision tree\n",
    "with no depth limits (and remaining default behavior) for a varying number of selected features\n",
    "in {5,10,40,100,250,700}. Feature selection should be performed before decision tree learning\n",
    "considering the discriminative power of the input variables according to mutual information\n",
    "criterion (mutual_info_classif)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import pandas as pd\n",
    "from scipy.io.arff import loadarff\n",
    "from sklearn import feature_selection, model_selection, tree, metrics, preprocessing\n",
    "import numpy as np\n",
    "\n",
    "# Reading the ARFF file\n",
    "data = loadarff(\"data/pd_speech.arff\")\n",
    "df = pd.DataFrame(data[0])\n",
    "df[\"class\"] = df[\"class\"].str.decode(\"utf-8\")\n",
    "\n",
    "X = df.drop(\"class\", axis=1)\n",
    "y = df[\"class\"]\n",
    "\n",
    "mutualInfoClassif = feature_selection.mutual_info_classif(X, y, random_state=1)\n",
    "\n",
    "values = {}\n",
    "\n",
    "for i in range(0, len(mutualInfoClassif)):\n",
    "    values[df.columns[i]] = mutualInfoClassif[i]\n",
    "\n",
    "values[\"0\"] = 1\n",
    "values[\"1\"] = 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X, y, train_size=0.7, stratify=y, random_state=1\n",
    ")\n",
    "\n",
    "NUM_FEATURES = [5, 10, 40, 100, 250, 700]\n",
    "\n",
    "training_accurancy = []\n",
    "test_accurancy = []\n",
    "\n",
    "for num_features in NUM_FEATURES:\n",
    "\n",
    "    values_sorted = {}\n",
    "    a = num_features\n",
    "\n",
    "    for column, info in sorted(values.items(), key=itemgetter(1), reverse=True):\n",
    "        if a > 0:\n",
    "            values_sorted[column] = info\n",
    "        else:\n",
    "            values_sorted[column] = 0\n",
    "        a -= 1\n",
    "\n",
    "    predictor = tree.DecisionTreeClassifier(class_weight=values_sorted)\n",
    "\n",
    "    predictor.fit(X_train, y_train)\n",
    "\n",
    "    y_train_pred = predictor.predict(X_train)\n",
    "    y_test_pred = predictor.predict(X_test)\n",
    "\n",
    "    train_acc = metrics.accuracy_score(y_train, y_train_pred)\n",
    "    test_acc = metrics.accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "    training_accurancy.append(train_acc)\n",
    "    test_accurancy.append(test_acc)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "plt.plot(NUM_FEATURES, training_accurancy, label=\"Training Accuraccy\")\n",
    "plt.plot(NUM_FEATURES, test_accurancy, label=\"Test Accuraccy\")\n",
    "\n",
    "plt.xlabel(\"Number of selected features\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why training accuracy is persistently 1? Critically analyze the gathered results"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}

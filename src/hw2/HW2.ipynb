{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework I\n",
    "\n",
    "Diogo Correia (ist199211) & Tomás Esteves (ist199341)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Pen and Paper [13v]\n",
    "\n",
    "**Four positive observations,{$ A \\choose 0 $, $ B \\choose 1 $, $ A \\choose 1 $, $ A \\choose 0 $} , and four negative observations, {$ B \\choose 0 $, $ B \\choose 0 $, $ A \\choose 1 $, $ B \\choose 1 $} were collected. Consider the problem of classifying observations as positive or negative.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) [4v] Compute the recall of a distance-weighted 𝑘NN with 𝑘 = 5 and distance 𝑑(𝐱1 , 𝐱2) = 𝐻𝑎𝑚𝑚𝑖𝑛𝑔(𝐱1 , 𝐱2) + $\\frac{1}{2}$ using leave-one-out evaluation schema (i.e., when observation, use all remaining ones)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**An additional positive observation was acquired, ( $𝐵 \\choose 0$), and a third variable 𝑦3 was independently monitored, yielding estimates 𝑦3|𝑃 = {1.2, 0.8, 0.5, 0.9,0.8} and 𝑦3|𝑁 = {1, 0.9, 1.2, 0.8}.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) [4v] Considering the nine training observations, learn a Bayesian classifier assuming:\n",
    "i) 𝑦1 and 𝑦2 are dependent\\\n",
    "ii) {𝑦1, 𝑦2} and {𝑦3} variable sets are independent and equally important\\\n",
    "ii) 𝑦3 is normally distributed. \n",
    "\n",
    "Show all parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Considering three testing observations, {((A, 1, 0.8), Positive), ((B, 1, 1), Positive), ((B, 0, 0.9), Negative)}**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) [3v] Under a MAP assumption, compute 𝑃(Positive|𝐱) of each testing observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) [2v] Given a binary class variable, the default decision threshold of 𝜃 = 0.5,\n",
    "f(x|𝜃) = Pos, if P(Positive|x) > 0\\\n",
    "Else negative\n",
    "\n",
    "can be adjusted. Which decision threshold – 0.3, 0.5 or 0.7 – optimizes testing accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming [7v]\n",
    "\n",
    "**Considering the `pd_speech.arff` dataset available at the homework tab:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) [3v]\n",
    "\n",
    "Using sklearn, considering a 10-fold stratified cross validation (random=0), plot the cumulative\n",
    "testing confusion matrices of 𝑘NN (uniform weights, 𝑘 = 5, Euclidean distance) and Naïve Bayes\n",
    "(Gaussian assumption). Use all remaining classifier parameters as default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import pandas as pd\n",
    "from scipy.io.arff import loadarff\n",
    "from sklearn import feature_selection, model_selection, tree, metrics, preprocessing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the ARFF file\n",
    "data = loadarff(\"../data/pd_speech.arff\")\n",
    "df = pd.DataFrame(data[0])\n",
    "df[\"class\"] = df[\"class\"].str.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features from the outcome (class)\n",
    "X = df.drop(\"class\", axis=1)\n",
    "y = df[\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = model_selection.StratifiedKFold(n_splits=10, random_state=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "predictor_knn = KNeighborsClassifier(\n",
    "    weights=\"uniform\", n_neighbors=5, metric=\"euclidean\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "predictor_naive_bayes = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_prob = []\n",
    "naive_bayes_prob = []\n",
    "\n",
    "for train_k, test_k in folds.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_k], X.iloc[test_k]\n",
    "    y_train, y_test = y.iloc[train_k], y.iloc[test_k]\n",
    "\n",
    "    predictor_knn.fit(X_train, y_train)\n",
    "    predictor_naive_bayes.fit(X_train, y_train)\n",
    "\n",
    "    knn_prob += predictor_knn.predict(X_test).tolist()\n",
    "    naive_bayes_prob += predictor_naive_bayes.predict(X_test).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "cm = np.array(confusion_matrix(y, knn_prob, labels=([\"0\", \"1\"])))\n",
    "\n",
    "confusion = pd.DataFrame(\n",
    "    cm,\n",
    "    index=[\"Parkinson\", \"Healthy\"],\n",
    "    columns=[\"Predicted Parkinson\", \"Predicted Healthy\"],\n",
    ")\n",
    "confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = np.array(confusion_matrix(y, naive_bayes_prob, labels=([\"0\", \"1\"])))\n",
    "\n",
    "confusion = pd.DataFrame(\n",
    "    cm,\n",
    "    index=[\"Parkinson\", \"Healthy\"],\n",
    "    columns=[\"Predicted Parkinson\", \"Predicted Healthy\"],\n",
    ")\n",
    "confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) [2v] Using scipy, test the hypothesis “𝑘NN is statistically superior to Naïve Bayes regarding accuracy”, asserting whether is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_prob = [int(item) for item in knn_prob]\n",
    "naive_bayes_prob = [int(item) for item in naive_bayes_prob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# predictor 1 is better than 2?\n",
    "res = stats.ttest_rel(knn_prob, naive_bayes_prob, alternative=\"greater\")\n",
    "print(\"knn > naive_bayes? pval=\", res.pvalue)\n",
    "\n",
    "# predictor 2 is better than 1?\n",
    "res = stats.ttest_rel(knn_prob, naive_bayes_prob, alternative=\"less\")\n",
    "print(\"knn < naive_bayes? pval=\", res.pvalue)\n",
    "\n",
    "# performance of predictor 1 differs from predictor 2?\n",
    "res = stats.ttest_rel(knn_prob, naive_bayes_prob, alternative=\"two-sided\")\n",
    "print(\"knn != naive_bayes? pval=\", res.pvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's false! i think"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) [2v] Enumerate three possible reasons that could underlie the observed differences in predictive accuracy between 𝑘NN and Naïve Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TODO"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}

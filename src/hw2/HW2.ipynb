{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework I\n",
    "\n",
    "Diogo Correia (ist199211) & TomÃ¡s Esteves (ist199341)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Pen and Paper [13v]\n",
    "\n",
    "**Four positive observations,{$ A \\choose 0 $, $ B \\choose 1 $, $ A \\choose 1 $, $ A \\choose 0 $} , and four negative observations, {$ B \\choose 0 $, $ B \\choose 0 $, $ A \\choose 1 $, $ B \\choose 1 $} were collected. Consider the problem of classifying observations as positive or negative.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) [4v] Compute the recall of a distance-weighted ð‘˜NN with ð‘˜ = 5 and distance ð‘‘(ð±1 , ð±2) = ð»ð‘Žð‘šð‘šð‘–ð‘›ð‘”(ð±1 , ð±2) + $\\frac{1}{2}$ using leave-one-out evaluation schema (i.e., when observation, use all remaining ones)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**An additional positive observation was acquired, ( $ðµ \\choose 0$), and a third variable ð‘¦3 was independently monitored, yielding estimates ð‘¦3|ð‘ƒ = {1.2, 0.8, 0.5, 0.9,0.8} and ð‘¦3|ð‘ = {1, 0.9, 1.2, 0.8}.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) [4v] Considering the nine training observations, learn a Bayesian classifier assuming:\n",
    "i) ð‘¦1 and ð‘¦2 are dependent\\\n",
    "ii) {ð‘¦1, ð‘¦2} and {ð‘¦3} variable sets are independent and equally important\\\n",
    "ii) ð‘¦3 is normally distributed. \n",
    "\n",
    "Show all parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Considering three testing observations, {((A, 1, 0.8), Positive), ((B, 1, 1), Positive), ((B, 0, 0.9), Negative)}**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) [3v] Under a MAP assumption, compute ð‘ƒ(Positive|ð±) of each testing observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) [2v] Given a binary class variable, the default decision threshold of ðœƒ = 0.5,\n",
    "f(x|ðœƒ) = Pos, if P(Positive|x) > 0\\\n",
    "Else negative\n",
    "\n",
    "can be adjusted. Which decision threshold â€“ 0.3, 0.5 or 0.7 â€“ optimizes testing accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming [7v]\n",
    "\n",
    "**Considering the `pd_speech.arff` dataset available at the homework tab:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) [3v]\n",
    "\n",
    "Using sklearn, considering a 10-fold stratified cross validation (random=0), plot the cumulative\n",
    "testing confusion matrices of ð‘˜NN (uniform weights, ð‘˜ = 5, Euclidean distance) and NaÃ¯ve Bayes\n",
    "(Gaussian assumption). Use all remaining classifier parameters as default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import pandas as pd\n",
    "from scipy.io.arff import loadarff\n",
    "from sklearn import feature_selection, model_selection, tree, metrics, preprocessing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the ARFF file\n",
    "data = loadarff(\"data/pd_speech.arff\")\n",
    "df = pd.DataFrame(data[0])\n",
    "df[\"class\"] = df[\"class\"].str.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features from the outcome (class)\n",
    "X = df.drop(\"class\", axis=1)\n",
    "y = df[\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into a training set (70%) and a testing set (30%)\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X, y, train_size=0.7, stratify=y, random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FEATURES = [5, 10, 40, 100, 250, 700]\n",
    "\n",
    "training_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "for num_features in NUM_FEATURES:\n",
    "    # Select the features with the greatest information gain (by mutual_info_classif)\n",
    "    kbest = feature_selection.SelectKBest(\n",
    "        score_func=lambda a, b: feature_selection.mutual_info_classif(\n",
    "            a, b, random_state=1\n",
    "        ),\n",
    "        k=num_features,\n",
    "    )\n",
    "    kbest.fit(X_train, y_train)\n",
    "\n",
    "    # Get the observation sets with only the selected N best features\n",
    "    X_train_cut = kbest.transform(X_train)\n",
    "    X_test_cut = kbest.transform(X_test)\n",
    "\n",
    "    # Fit the decision tree classifier\n",
    "    predictor = tree.DecisionTreeClassifier(random_state=1)\n",
    "    predictor.fit(X_train_cut, y_train)\n",
    "\n",
    "    # Use the decision tree to predict the outcome of the given observations\n",
    "    y_train_pred = predictor.predict(X_train_cut)\n",
    "    y_test_pred = predictor.predict(X_test_cut)\n",
    "\n",
    "    # Get the accuracy of each test\n",
    "    train_acc = metrics.accuracy_score(y_train, y_train_pred)\n",
    "    test_acc = metrics.accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "    training_accuracy.append(train_acc)\n",
    "    test_accuracy.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "    NUM_FEATURES,\n",
    "    training_accuracy,\n",
    "    label=\"Training Accuracy\",\n",
    "    marker=\"+\",\n",
    "    color=\"#4caf50\",\n",
    ")\n",
    "plt.plot(\n",
    "    NUM_FEATURES, test_accuracy, label=\"Test Accuracy\", marker=\".\", color=\"#ff5722\"\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Number of Selected Features\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(\"../../report/assets/hw1-plot.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) [2v] Using scipy, test the hypothesis â€œð‘˜NN is statistically superior to NaÃ¯ve Bayes regarding accuracyâ€, asserting whether is true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) [2v] Enumerate three possible reasons that could underlie the observed differences in predictive accuracy between ð‘˜NN and NaÃ¯ve Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TODO"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}

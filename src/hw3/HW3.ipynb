{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework III\n",
    "\n",
    "Diogo Correia (ist199211) & TomÃ¡s Esteves (ist199341)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Pen and Paper [12v]\n",
    "\n",
    "**Consider the problem of learning a regression model from 5 univariate observations ((0.8), (1), (1.2), (1.4), (1.6)) with targets (24,20,10,13,12).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv(\"../data/hw3data.csv\")\n",
    "\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) [5v] Consider the basis function, $\\phi_j(x) = x^j$ , for performing a 3-order polynomial regression,\n",
    "\n",
    "$$\n",
    "\\hat{z} (x,w) = \\sum_{j=0}^3 w_j \\phi_j(x) = w_0 + w_1 x + w_2 x^2 + w_3 x^3\n",
    "$$\n",
    "\n",
    "Learn the Ridge regression ($l_2$ regularization) on the transformed data space using the closed form solution with $\\lambda = 2$.\n",
    "\n",
    "Hint: use numpy matrix operations (e.g., linalg.pinv for inverse) to validate your calculus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.linalg as la\n",
    "import numpy as np\n",
    "from numpyarray_to_latex import to_ltx\n",
    "from numpyarray_to_latex.jupyter import to_jup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ltx(array, **wargs):\n",
    "    print(to_ltx(array, latexarraytype=\"bmatrix\", **wargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jup(array, **wargs):\n",
    "    to_jup(array, fmt=\"{:6.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x = df1.drop(\"target\", axis=1)\n",
    "target = np.asmatrix(df1[\"target\"]).transpose()\n",
    "\n",
    "df_x[\"y0\"] = df_x[\"y1\"] ** 0\n",
    "df_x[\"y2\"] = df_x[\"y1\"] ** 2\n",
    "df_x[\"y3\"] = df_x[\"y1\"] ** 3\n",
    "\n",
    "# swap columns\n",
    "y1 = np.asarray(df_x[[\"y0\", \"y1\", \"y2\", \"y3\"]])\n",
    "ltx(y1)\n",
    "to_jup(y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_transposed = y1.transpose()\n",
    "ltx(y1_transposed)\n",
    "to_jup(y1_transposed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1t_y1 = np.matmul(y1_transposed, y1)\n",
    "ltx(y1t_y1)\n",
    "to_jup(y1t_y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1t_y1_with_lambda = y1t_y1 + 2 * np.identity(4)\n",
    "ltx(\n",
    "    y1t_y1_with_lambda,\n",
    "    mark_color=\"bblue\",\n",
    "    mark_elements=[\n",
    "        (0, 0),\n",
    "        (1, 1),\n",
    "        (2, 2),\n",
    "        (3, 3),\n",
    "    ],\n",
    ")\n",
    "to_jup(y1t_y1_with_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted = np.linalg.inv(y1t_y1_with_lambda)\n",
    "ltx(inverted)\n",
    "to_jup(inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_mult = np.matmul(inverted, y1_transposed)\n",
    "ltx(inverted_mult)\n",
    "to_jup(inverted_mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.matmul(inverted_mult, target)\n",
    "ltx(w, mark_elements=[(i, 0) for i in range(4)])\n",
    "to_jup(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"W0: \", w[0])\n",
    "print(\"W1: \", w[1])\n",
    "print(\"W2: \", w[2])\n",
    "print(\"W3: \", w[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) [1v] Compute the training RMSE for the learnt regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.matmul(y1, w)\n",
    "ltx(pred)\n",
    "to_jup(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_jup(pred - target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = np.multiply((pred - target), (pred - target))\n",
    "ltx(distances)\n",
    "to_jup(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_sum = distances.sum()\n",
    "distances_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(distances_sum / 5)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) [6v] Consider a multi-layer perceptron characterized by one hidden layer with 2 nodes. \n",
    "\n",
    "Using the activation function $f(x) = e^{0.1x}$ on all units, all weights initialized as 1 (including biases), and the half squared error loss.\n",
    "\n",
    "Perform one batch gradient descent update (with learning rate $ \\eta = 0.1$) for the first three observations (0.8), (1) and (1.2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_target = df1[\"target\"]\n",
    "\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "W^{[1]} =  \\begin{pmatrix}\n",
    "1\\\\\n",
    "1\n",
    "\\end{pmatrix}\n",
    "\n",
    "W^{[2]} =  \\begin{pmatrix}\n",
    "1 & 1\\\\\n",
    "\\end{pmatrix}\n",
    "\n",
    "b^{[1]} =  \\begin{pmatrix}\n",
    "1\\\\\n",
    "1\n",
    "\\end{pmatrix}\n",
    "\n",
    "b^{[2]} =  \\begin{pmatrix}\n",
    "1\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "weight_1 = np.array([[1], [1]])\n",
    "weight_2 = np.array([[1, 1]])\n",
    "\n",
    "bias_1 = np.array([[1], [1]])\n",
    "bias_2 = np.array([[1]])\n",
    "\n",
    "y_values = np.array([df1[\"y1\"][0:3]])\n",
    "y_targets = np.array([df1[\"target\"][0:3]])\n",
    "\n",
    "to_jup(y_values)\n",
    "to_jup(y_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "z^{[1]} = W^{[1]} \\cdot x + b^{[1]}\\\\\n",
    "\n",
    "x^{[1]} = f(z^{[1]})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_1(x1):\n",
    "    return np.dot(weight_1, x1) + bias_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_1_values = z_1(y_values)\n",
    "\n",
    "jup(z_1_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return np.e ** (0.1 * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1_values = f(z_1_values)\n",
    "\n",
    "jup(x_1_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "z^{[2]} = W^{[2]} \\cdot x + b^{[2]}\\\\\n",
    "\n",
    "x^{[2]} = f(z^{[2]})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_2(x2):\n",
    "    return np.matmul(weight_2, x2) + bias_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_2_values = z_2(x_1_values)\n",
    "\n",
    "jup(z_2_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2_values = f(z_2_values)\n",
    "\n",
    "jup(x_2_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\n",
    "E(x^{[2]},t) = \\cfrac{1}{2}(x^{[2]} - t)^2\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\cfrac{\\delta E}{\\delta x^{[2]}} = \\cfrac{1}{2}(2x^{[2]} - 2t) = x^{[2]} - t\\\\\n",
    "\n",
    "\\cfrac{\\delta x^{[i]}}{\\delta z^{[i]}} = f'(z^{[i]}) = 0.1e^{0.1z^{[i]}}\\\\\n",
    "\n",
    "\\cfrac{\\delta z^{[i]}}{\\delta W^{[i]}} = x^{[i-1]}\\\\\n",
    "\n",
    "\\cfrac{\\delta z^{[i]}}{\\delta b^{[i]}} = 1\\\\\n",
    "\n",
    "\\cfrac{\\delta z^{[i]}}{\\delta x^{[i-1]}} = W^{[i]}\\\\\n",
    "\n",
    "\\cfrac{\\delta E}{\\delta W^{[i]}} = \\delta^{[i]} \\cfrac{\\delta z^{[i]}}{\\delta W^{[i]}} = \\delta^{[i]} (x^{[i-1]})^T\\\\\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\n",
    "\\text{Output layer:}\\\\\n",
    "\n",
    "\\delta^{[2]} = \\cfrac{\\delta E}{\\delta x^{[2]}} \\cfrac{\\delta x^{[2]}}{\\delta z^{[2]}}\\\\\n",
    "\n",
    "\\text {Other layers:}\\\\\n",
    "\n",
    "\\delta^{[i]} = (\\cfrac{\\delta z^{[i+1]}}{\\delta x^{[i]}})^T \\delta^{[i+1]} \\cfrac{\\delta x^{[i]}}{\\delta z^{[i]}} \n",
    "\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\n",
    "\\delta^{[2]} = (x^{[2]} - t) \\circ 0.1e^{0.1z^{[2]}}\\\\\n",
    "\n",
    "\\delta^{[1]} = (W^{[2]})^T \\delta^{[2]} \\circ 0.1e^{0.1z^{[1]}}\\\\\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_derivative(x):\n",
    "    return 0.1 * np.e ** (0.1 * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_2_fn(x_2, t, z_2):\n",
    "    return (x_2 - t) * 0.1 * np.e ** (0.1 * z_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_n_fn(w_n_plus_1, delta_n_plus_1, z_n):\n",
    "    return np.multiply((w_n_plus_1.transpose() * delta_n_plus_1), f_derivative(z_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_2_values = delta_2_fn(x_2_values, y_targets, z_2_values)\n",
    "\n",
    "jup(delta_2_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_1_values = delta_n_fn(weight_2, delta_2_values, z_1_values)\n",
    "\n",
    "jup(delta_1_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "W_{new}^{[i]} = W_{old}^{[i]} - \\eta \\cfrac{\\delta E} {\\delta w^{[i]}} = W_{old}^{[i]}- \\eta \\delta^{[i]} (x^{[i-1]})^T\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_fn(w_old_n, eta, delta_n, x_n_minus_1):\n",
    "    return w_old_n - eta * np.matmul(delta_n, x_n_minus_1.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_delta_weights(learning_rate, delta_n, x_n_minus_1):\n",
    "    for i in range(delta_n.shape[1]):\n",
    "        jup(\n",
    "            -learning_rate * np.matmul(delta_n[:, [i]], x_n_minus_1[:, [i]].transpose())\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_delta_weights(learning_rate, delta_2_values, x_1_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weight_2 = weight_fn(weight_2, learning_rate, delta_2_values, x_1_values)\n",
    "\n",
    "jup(new_weight_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_delta_weights(learning_rate, delta_1_values, y_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weight_1 = weight_fn(weight_1, learning_rate, delta_1_values, y_values)\n",
    "\n",
    "jup(new_weight_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "b_{new}^{[i]} = b_{old}^{[i]} - \\eta \\cfrac{\\delta E} {\\delta b^{[i]}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_fn(b_old_n, eta, delta_n):\n",
    "    return b_old_n - eta * np.matmul(\n",
    "        delta_n, np.vstack(np.repeat([1], delta_1_values.shape[1]))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_delta_biases(learning_rate, delta_n):\n",
    "    for i in range(delta_n.shape[1]):\n",
    "        jup(-learning_rate * delta_n[:, [i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_delta_biases(learning_rate, delta_2_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_bias_2 = bias_fn(bias_2, learning_rate, delta_2_values)\n",
    "\n",
    "jup(new_bias_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_delta_biases(learning_rate, delta_1_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_bias_1 = bias_fn(bias_1, learning_rate, delta_1_values)\n",
    "\n",
    "jup(new_bias_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Propagation\n",
    "$$\n",
    "W^{[1]} =  \\begin{pmatrix}\n",
    "1.08062041\\\\\n",
    "1.08062041\n",
    "\\end{pmatrix}\n",
    "\n",
    "W^{[2]} =  \\begin{pmatrix}\n",
    "1.8518485 & 1.8518485\\\\\n",
    "\\end{pmatrix}\n",
    "\n",
    "b^{[1]} =  \\begin{pmatrix}\n",
    "1.08518485\\\\\n",
    "1.08518485\n",
    "\\end{pmatrix}\n",
    "\n",
    "b^{[2]} =  \\begin{pmatrix}\n",
    "1.7012589\n",
    "\\end{pmatrix}\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming and Critical Anlaysis[8v]\n",
    "\n",
    "**Consider the following three regressors applied on kin8nm.arff data (available at the webpage):**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- linear regression with Ridge regularization term of 0.1\n",
    "- two MLPs\n",
    "     - ð‘€ð¿ð‘ƒ1 and ð‘€ð¿ð‘ƒ2 \n",
    "- each with two hidden layers of size 10, hyperbolic tangent function as the activation function of all nodes, a maximum of 500 iterations, and a fixed seed (random_state=0). \n",
    "- ð‘€ð¿ð‘ƒ1 should be parameterized with early stopping while ð‘€ð¿ð‘ƒ2 should not consider early stopping. \n",
    "\n",
    "Remaining parameters (e.g., loss function, batch size, regularization term, solver) should be set as default\n",
    "\n",
    "Using a 70-30 training-test split with a fixed seed (random_state=0):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) [4v] **Compute the MAE of the three regressors: linear regression, ð‘€ð¿ð‘ƒ1 and ð‘€ð¿ð‘ƒ2.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io.arff import loadarff\n",
    "import pandas as pd\n",
    "from sklearn import model_selection, metrics\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the ARFF file\n",
    "data = loadarff(\"../data/kin8nm.arff\")\n",
    "df = pd.DataFrame(data[0])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features from the outcome (class)\n",
    "X = df.drop(\"y\", axis=1)\n",
    "y = df[\"y\"]\n",
    "\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into a training set (70%) and a testing set (30%)\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X.values, y.values, train_size=0.7, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr = Ridge(alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp1 = MLPRegressor(\n",
    "    hidden_layer_sizes=(10, 10),\n",
    "    activation=\"tanh\",\n",
    "    max_iter=500,\n",
    "    random_state=0,\n",
    "    early_stopping=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp2 = MLPRegressor(\n",
    "    hidden_layer_sizes=(10, 10),\n",
    "    activation=\"tanh\",\n",
    "    max_iter=500,\n",
    "    random_state=0,\n",
    "    early_stopping=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr.fit(X_train, y_train)\n",
    "mlp1.fit(X_train, y_train)\n",
    "mlp2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_pred = rr.predict(X_test)\n",
    "mlp1_pred = mlp1.predict(X_test)\n",
    "mlp2_pred = mlp2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ridge Regularization MAE:\", metrics.mean_absolute_error(y_test, rr_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MLP1 Regularization MAE:\", metrics.mean_absolute_error(y_test, mlp1_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MLP2 Regularization MAE:\", metrics.mean_absolute_error(y_test, mlp2_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) [1.5v] **Plot the residues (in absolute value) using two visualizations: boxplots and histograms.**\n",
    "\n",
    "Hint: consider using boxplot and hist functions from matplotlib.pyplot to this end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_residues = []\n",
    "mlp1_residues = []\n",
    "mlp2_residues = []\n",
    "\n",
    "for i in range(0, len(y_test)):\n",
    "    rr_residues.append(abs(y_test[i] - rr_pred[i]))\n",
    "    mlp1_residues.append(abs(y_test[i] - mlp1_pred[i]))\n",
    "    mlp2_residues.append(abs(y_test[i] - mlp2_pred[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"Ridge\": rr_residues, \"MLP1\": mlp1_residues, \"MLP2\": mlp2_residues})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) [1v] **How many iterations were required for ð‘€ð¿ð‘ƒ1 and ð‘€ð¿ð‘ƒ2 to converge?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MLP1 number of iterations:\", mlp1.n_iter_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MLP2 number of iterations:\", mlp2.n_iter_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) [1.5v] **What can be motivating the unexpected differences on the number of iterations?**\n",
    "\n",
    "**Hypothesize one reason underlying the observed performance differences between the MLPs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more about the MLP regressor at https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By comparing the number of iteration of both models, we can conclude that MLP1 takes a lot more iterations (452 > 77) to converge than MLP2. This is because MLP1 uses early stopping, which means that the model stops training when the validation score is not improving anymore. By reading the documentation about the [sklearn.neural_network.MLPRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html), we can see that the default value for the validation fraction is `validation_fraction` (default = 0.1), which means that 10% of the training data is used for validation and the other 90% data to train the model. After each iteration, the model will calculate the validation score. If the score is not improving by at least `tol` (default value = 1e-4) for `n_iter_no_change` (default value = 10), the model will stop training. This will allow the MLP1 to have better performance (lower MAE value, and lower residue values) than MLP2, but it will take a lot more iterations to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the MLP1 uses early stopping and MLP2 does not, we can attribute the difference in performance to the early stopping. \n",
    "Due to how the early stopping works, similar to splitting the data into training and testing, early stopping splits the training data into training and validation, this will train a model that will learn from the training data and will evaluate it with the valuation data. This will cause the model to learn more effectively, which will cause the model to be more robust and generalize better. This is why the MLP1 has a lower MAE value and lower residue values than MLP2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this takes 1m32sec to run on my machine, so don't run it unless you have time to spare :P\n",
    "\n",
    "validation_scores = []\n",
    "for n_iter in range(1, 500, 10):\n",
    "    mlp = MLPRegressor(\n",
    "        hidden_layer_sizes=(10, 10),\n",
    "        activation=\"tanh\",\n",
    "        max_iter=n_iter,\n",
    "        random_state=0,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "    mlp.fit(X_train, y_train)\n",
    "    validation_scores.append(mlp.score(X_train, y_train))\n",
    "\n",
    "validation_scores"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}

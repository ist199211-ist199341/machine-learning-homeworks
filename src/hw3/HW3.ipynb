{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework III\n",
    "\n",
    "Diogo Correia (ist199211) & Tom√°s Esteves (ist199341)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Pen and Paper [12v]\n",
    "\n",
    "**Consider the problem of learning a regression model from 5 univariate observations ((0.8), (1), (1.2), (1.4), (1.6)) with targets (24,20,10,13,12).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv(\"../data/hw3data.csv\")\n",
    "\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) [5v] Consider the basis function, $\\phi_j(x) = x^j$ , for performing a 3-order polynomial regression,\n",
    "\n",
    "$$\n",
    "\\hat{z} (x,w) = \\sum_{j=0}^3 w_j \\phi_j(x) = w_0 + w_1 x + w_2 x^2 + w_3 x^3\n",
    "$$\n",
    "\n",
    "Learn the Ridge regression ($l_2$ regularization) on the transformed data space using the closed form solution with $\\lambda = 2$.\n",
    "\n",
    "Hint: use numpy matrix operations (e.g., linalg.pinv for inverse) to validate your calculus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.linalg as la\n",
    "import numpy as np\n",
    "\n",
    "y1 = df1.drop(\"target\", axis=1)\n",
    "target = df1[\"target\"]\n",
    "\n",
    "y1[\"y0\"] = y1[\"y1\"] ** 0\n",
    "y1[\"y2\"] = y1[\"y1\"] ** 2\n",
    "y1[\"y3\"] = y1[\"y1\"] ** 3\n",
    "\n",
    "# swap columns\n",
    "y1 = y1[[\"y0\", \"y1\", \"y2\", \"y3\"]]\n",
    "\n",
    "y1_y1t = np.matmul(y1.transpose(), y1)\n",
    "\n",
    "inverted = np.linalg.inv((y1_y1t + 2 * np.identity(4)))\n",
    "\n",
    "inverted_mult = np.matmul(inverted, y1.transpose())\n",
    "\n",
    "w = np.matmul(inverted_mult, target)\n",
    "\n",
    "print(\"W0: \", w[0])\n",
    "print(\"W1: \", w[1])\n",
    "print(\"W2: \", w[2])\n",
    "print(\"W3: \", w[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) [1v] Compute the training RMSE for the learnt regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z(x1):\n",
    "    return np.matmul(w.transpose(), np.array([1, x1, x1**2, x1**3]))\n",
    "\n",
    "\n",
    "pred = list(map(z, y1[\"y1\"]))\n",
    "\n",
    "sum = 0\n",
    "\n",
    "for i in range(0, 5):\n",
    "    print(y1[\"y1\"][i], target[i], pred[i])\n",
    "    sum += (target[i] - pred[i]) ** 2\n",
    "\n",
    "rmse = np.sqrt(sum / 5)\n",
    "\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) [6v] Consider a multi-layer perceptron characterized by one hidden layer with 2 nodes. \n",
    "\n",
    "Using the activation function $f(x) = e^{0.1x}$ on all units, all weights initialized as 1 (including biases), and the half squared error loss.\n",
    "\n",
    "Perform one batch gradient descent update (with learning rate $ \\eta = 0.1$) for the first three observations (0.8), (1) and (1.2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_target = df1[\"target\"]\n",
    "\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Propagation\n",
    "$$\n",
    "W^{[1]} =  \\begin{pmatrix}\n",
    "1\\\\\n",
    "1\n",
    "\\end{pmatrix}\n",
    "\n",
    "W^{[2]} =  \\begin{pmatrix}\n",
    "1 & 1\\\\\n",
    "\\end{pmatrix}\n",
    "\n",
    "b^{[1]} =  \\begin{pmatrix}\n",
    "1\\\\\n",
    "1\n",
    "\\end{pmatrix}\n",
    "\n",
    "b^{[2]} =  \\begin{pmatrix}\n",
    "1\n",
    "\\end{pmatrix}\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "z^{[1]} = W^{[1]} \\cdot x + b^{[1]}\\\\\n",
    "\n",
    "x^{[1]} = f(z^{[1]})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "z^{[2]} = W^{[2]} \\cdot x + b^{[2]}\\\\\n",
    "\n",
    "x^{[2]} = f(z^{[2]})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_1 = np.array([[1], [1]])\n",
    "weight_2 = np.array([1, 1])\n",
    "\n",
    "bias_1 = np.array([1, 1])\n",
    "bias_2 = np.array([1])\n",
    "\n",
    "y_values = np.array([[0.8], [1], [1.2]])\n",
    "\n",
    "y_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_1(x1):\n",
    "    return np.matmul(weight_1, np.array(x1).transpose()) + bias_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_1_values = list(map(z_1, y_values))\n",
    "\n",
    "z_1_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return np.e ** (0.1 * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1_values = list(map(f, z_1_values))\n",
    "\n",
    "x_1_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_2(x2):\n",
    "    return np.matmul(weight_2, np.array(x2).transpose()) + bias_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_2_values = list(map(z_2, x_1_values))\n",
    "\n",
    "z_2_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2_values = list(map(f, z_2_values))\n",
    "\n",
    "x_2_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = y_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\n",
    "E(x^{[2]},t) = \\cfrac{1}{2}(x^{[2]} - t)^2\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\cfrac{\\delta E}{\\delta x^{[2]}} = \\cfrac{1}{2}(2x^{[2]} - 2t) = x^{[2]} - t\\\\\n",
    "\n",
    "\\cfrac{\\delta x^{[i]}}{\\delta z^{[i]}} = f'(z^{[i]}) = 0.1e^{0.1z^{[i]}}\\\\\n",
    "\n",
    "\\cfrac{\\delta z^{[i]}}{\\delta W^{[i]}} = x^{[i-1]}\\\\\n",
    "\n",
    "\\cfrac{\\delta z^{[i]}}{\\delta b^{[i]}} = 1\\\\\n",
    "\n",
    "\\cfrac{\\delta z^{[i]}}{\\delta x^{[i-1]}} = W^{[i]}\\\\\n",
    "\n",
    "\\cfrac{\\delta E}{\\delta W^{[i]}} = \\delta^{[i]} \\cfrac{\\delta z^{[i]}}{\\delta W^{[i]}} = \\delta^{[i]} (x^{[i-1]})^T\\\\\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\n",
    "\\text{Output layer:}\\\\\n",
    "\n",
    "\\delta^{[2]} = \\cfrac{\\delta E}{\\delta x^{[2]}} \\cfrac{\\delta x^{[2]}}{\\delta z^{[2]}}\\\\\n",
    "\n",
    "\\text {Other layers:}\\\\\n",
    "\n",
    "\\delta^{[i]} = (\\cfrac{\\delta z^{[i+1]}}{\\delta x^{[i]}})^T \\delta^{[i+1]} \\cfrac{\\delta x^{[i]}}{\\delta z^{[i]}} \n",
    "\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\n",
    "\\delta^{[2]} = (x^{[2]} - t) \\cdot 0.1e^{0.1z^{[2]}}\\\\\n",
    "\n",
    "\\delta^{[1]} = (W^{[2]})^T \\delta^{[2]} \\cdot 0.1e^{0.1z^{[1]}}\\\\\n",
    "\n",
    "\\delta^{[0]} = (W^{[1]})^T \\delta^{[1]} \\cdot 0.1e^{0.1z^{[0]}}\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_derivative(x):\n",
    "    return 0.1 * np.e ** (0.1 * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_2_fn(x_2, t, z_2):\n",
    "    return (x_2 - t) * 0.1 * np.e ** (0.1 * z_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_n_fn(w_n_plus_1, delta_n_plus_1, z_n):\n",
    "    return np.multiply(\n",
    "        (w_n_plus_1.transpose() * delta_n_plus_1), np.vectorize(f_derivative)(z_n)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_2_values = []\n",
    "\n",
    "for i in range(0, 3):\n",
    "    delta_2_values.append(delta_2_fn(x_2_values[i], df1_target[i], z_2_values[i]))\n",
    "\n",
    "delta_2_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_1_values = delta_n_fn(weight_2, delta_2_values, np.matrix(z_1_values))\n",
    "\n",
    "delta_1_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming and Critical Anlaysis[8v]\n",
    "\n",
    "**Consider the following three regressors applied on kin8nm.arff data (available at the webpage):**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- linear regression with Ridge regularization term of 0.1\n",
    "- two MLPs\n",
    "     - ùëÄùêøùëÉ1 and ùëÄùêøùëÉ2 \n",
    "- each with two hidden layers of size 10, hyperbolic tangent function as the activation function of all nodes, a maximum of 500 iterations, and a fixed seed (random_state=0). \n",
    "- ùëÄùêøùëÉ1 should be parameterized with early stopping while ùëÄùêøùëÉ2 should not consider early stopping. \n",
    "\n",
    "Remaining parameters (e.g., loss function, batch size, regularization term, solver) should be set as default\n",
    "\n",
    "Using a 70-30 training-test split with a fixed seed (random_state=0):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) [4v] **Compute the MAE of the three regressors: linear regression, ùëÄùêøùëÉ1 and ùëÄùêøùëÉ2.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import pandas as pd\n",
    "from scipy.io.arff import loadarff\n",
    "from sklearn import feature_selection, model_selection, tree, metrics, preprocessing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the ARFF file\n",
    "data = loadarff(\"../data/kin8nm.arff\")\n",
    "df = pd.DataFrame(data[0])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features from the outcome (class)\n",
    "X = df.drop(\"y\", axis=1)\n",
    "y = df[\"y\"]\n",
    "\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into a training set (70%) and a testing set (30%)\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X.values, y.values, train_size=0.7, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr = Ridge(alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp1 = MLPRegressor(\n",
    "    hidden_layer_sizes=(10, 10),\n",
    "    activation=\"tanh\",\n",
    "    max_iter=500,\n",
    "    random_state=0,\n",
    "    early_stopping=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp2 = MLPRegressor(\n",
    "    hidden_layer_sizes=(10, 10),\n",
    "    activation=\"tanh\",\n",
    "    max_iter=500,\n",
    "    random_state=0,\n",
    "    early_stopping=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr.fit(X_train, y_train)\n",
    "mlp1.fit(X_train, y_train)\n",
    "mlp2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_pred = rr.predict(X_test)\n",
    "mlp1_pred = mlp1.predict(X_test)\n",
    "mlp2_pred = mlp2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ridge Regularization MAE:\", metrics.mean_absolute_error(y_test, rr_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MLP1 Regularization MAE:\", metrics.mean_absolute_error(y_test, mlp1_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MLP2 Regularization MAE:\", metrics.mean_absolute_error(y_test, mlp2_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) [1.5v] **Plot the residues (in absolute value) using two visualizations: boxplots and histograms.**\n",
    "\n",
    "Hint: consider using boxplot and hist functions from matplotlib.pyplot to this end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_residues = []\n",
    "mlp1_residues = []\n",
    "mlp2_residues = []\n",
    "\n",
    "for i in range(0, len(y_test)):\n",
    "    rr_residues.append(abs(y_test[i] - rr_pred[i]))\n",
    "    mlp1_residues.append(abs(y_test[i] - mlp1_pred[i]))\n",
    "    mlp2_residues.append(abs(y_test[i] - mlp2_pred[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"Ridge\": rr_residues, \"MLP1\": mlp1_residues, \"MLP2\": mlp2_residues})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) [1v] **How many iterations were required for ùëÄùêøùëÉ1 and ùëÄùêøùëÉ2 to converge?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MLP1 number of iterations:\", mlp1.n_iter_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MLP2 number of iterations:\", mlp2.n_iter_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) [1.5v] **What can be motivating the unexpected differences on the number of iterations?**\n",
    "\n",
    "**Hypothesize one reason underlying the observed performance differences between the MLPs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more about the MLP regressor at https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}

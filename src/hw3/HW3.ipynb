{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework III\n",
    "\n",
    "Diogo Correia (ist199211) & Tom√°s Esteves (ist199341)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Pen and Paper [12v]\n",
    "\n",
    "**Given the following decision tree learnt from 20 observation using Shannon entropy, with leaf annotations (`#correct/#total`)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Decision Tree](./decision_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) [4v] Draw the training confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <td colspan=\"2\" rowspan=\"2\" style=\"border-top: none; border-left: none;\"></td>\n",
    "    <th colspan=\"2\">True</th>\n",
    "    <td rowspan=\"2\" style=\"border-top: none; border-right: none;\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Positive</th>\n",
    "    <th>Negative</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th rowspan=\"2\">Predicted</th>\n",
    "    <th>Positive</th>\n",
    "    <td>8</td>\n",
    "    <td>4</td>\n",
    "    <td>12</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Negative</th>\n",
    "    <td>3</td>\n",
    "    <td>5</td>\n",
    "    <td>8</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th colspan=\"2\" style=\"border-left: none; border-bottom: none;\"></th>\n",
    "    <td>11</td>\n",
    "    <td>9</td>\n",
    "    <td>20</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) [3v] Identify the training F1 after a post-pruning of the given tree under a maximum depth of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <td colspan=\"2\" rowspan=\"2\" style=\"border-top: none; border-left: none;\"></td>\n",
    "    <th colspan=\"2\">True</th>\n",
    "    <td rowspan=\"2\" style=\"border-top: none; border-right: none;\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Positive</th>\n",
    "    <th>Negative</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th rowspan=\"2\">Predicted</th>\n",
    "    <th>Positive</th>\n",
    "    <td>5</td>\n",
    "    <td>2</td>\n",
    "    <td>7</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Negative</th>\n",
    "    <td>6</td>\n",
    "    <td>7</td>\n",
    "    <td>13</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th colspan=\"2\" style=\"border-left: none; border-bottom: none;\"></th>\n",
    "    <td>11</td>\n",
    "    <td>9</td>\n",
    "    <td>20</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positives = 5\n",
    "false_positives = 2\n",
    "false_negatives = 6\n",
    "\n",
    "precision = true_positives / (true_positives + false_positives)\n",
    "recall = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "f1_measure = (0.5 * (1 / precision + 1 / recall)) ** (-1)\n",
    "\n",
    "f1_measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) [2v] Identify two different reasons as to why the left tree path was not further decomposed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left tree path might not have been further decomposed because:\n",
    "\n",
    "- We did not want to overfit the model, since we have a very small samlpe size.\n",
    "  For this reason, if we were to further decompose the left tree path, we might end up with a less accurate\n",
    "  decision tree, since the 2 negative observations might have been outliers.\n",
    "- The information gain of this branch, $IG(y_{out} | y_2, y_1 = A)$, might be very small,\n",
    "  since there are a lot more observations classified as positive than as negative.\n",
    "  If we were to decompose the left path, there might be no optimal division that would correctly identify all observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) [3v] Compute the information gain of variable y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log2\n",
    "import operator as op\n",
    "from itertools import chain\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT\n",
    "total_positive_count = 11\n",
    "total_negative_count = 9\n",
    "\n",
    "branch_a_positive_count = 5\n",
    "branch_a_negative_count = 2\n",
    "\n",
    "branch_b_positive_count = 6\n",
    "branch_b_negative_count = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def entropy_by_count(counts):\n",
    "    \"\"\"\n",
    "    Calculates the information entropy, I(X), of a set, given the count of each class of element\n",
    "    \"\"\"\n",
    "    total = sum(counts)\n",
    "    return reduce(op.add, map(lambda x: -(x / total) * log2(x / total), counts))\n",
    "\n",
    "\n",
    "def split_entropy_by_count(branch_counts):\n",
    "    \"\"\"\n",
    "    Calculates the entropy after branching on a variable\n",
    "    \"\"\"\n",
    "    # branch counts is a list of int lists\n",
    "    total = sum(chain(*branch_counts))\n",
    "    return reduce(\n",
    "        op.add, map(lambda x: (sum(x) / total) * entropy_by_count(x), branch_counts)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_y_out = entropy_by_count([total_positive_count, total_negative_count])\n",
    "entropy_y_out_y1 = split_entropy_by_count(\n",
    "    [\n",
    "        [branch_a_positive_count, branch_a_negative_count],\n",
    "        [branch_b_positive_count, branch_b_negative_count],\n",
    "    ]\n",
    ")\n",
    "\n",
    "information_gain = entropy_y_out - entropy_y_out_y1\n",
    "\n",
    "information_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming and Critical Anlaysis[8v]\n",
    "\n",
    "**Consider the following three regressors applied on kin8nm.arff data (available at the webpage):**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- linear regression with Ridge regularization term of 0.1\n",
    "- two MLPs\n",
    "     - ùëÄùêøùëÉ1 and ùëÄùêøùëÉ2 \n",
    "- each with two hidden layers of size 10, hyperbolic tangent function as the activation function of all nodes, a maximum of 500 iterations, and a fixed seed (random_state=0). \n",
    "- ùëÄùêøùëÉ1 should be parameterized with early stopping while ùëÄùêøùëÉ2 should not consider early stopping. \n",
    "\n",
    "Remaining parameters (e.g., loss function, batch size, regularization term, solver) should be set as default\n",
    "\n",
    "Using a 70-30 training-test split with a fixed seed (random_state=0):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) [4v] **Compute the MAE of the three regressors: linear regression, ùëÄùêøùëÉ1 and ùëÄùêøùëÉ2.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import pandas as pd\n",
    "from scipy.io.arff import loadarff\n",
    "from sklearn import feature_selection, model_selection, tree, metrics, preprocessing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the ARFF file\n",
    "data = loadarff(\"../data/kin8nm.arff\")\n",
    "df = pd.DataFrame(data[0])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features from the outcome (class)\n",
    "X = df.drop(\"y\", axis=1)\n",
    "y = df[\"y\"]\n",
    "\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into a training set (70%) and a testing set (30%)\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X.values, y.values, train_size=0.7, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr = Ridge(alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp1 = MLPRegressor(\n",
    "    hidden_layer_sizes=(10, 10),\n",
    "    activation=\"tanh\",\n",
    "    max_iter=500,\n",
    "    random_state=0,\n",
    "    early_stopping=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp2 = MLPRegressor(\n",
    "    hidden_layer_sizes=(10, 10),\n",
    "    activation=\"tanh\",\n",
    "    max_iter=500,\n",
    "    random_state=0,\n",
    "    early_stopping=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr.fit(X_train, y_train)\n",
    "mlp1.fit(X_train, y_train)\n",
    "mlp2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_pred = rr.predict(X_test)\n",
    "mlp1_pred = mlp1.predict(X_test)\n",
    "mlp2_pred = mlp2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ridge Regularization MAE:\", metrics.mean_absolute_error(y_test, rr_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MLP1 Regularization MAE:\", metrics.mean_absolute_error(y_test, mlp1_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MLP2 Regularization MAE:\", metrics.mean_absolute_error(y_test, mlp2_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) [1.5v] **Plot the residues (in absolute value) using two visualizations: boxplots and histograms.**\n",
    "\n",
    "Hint: consider using boxplot and hist functions from matplotlib.pyplot to this end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_residues = []\n",
    "mlp1_residues = []\n",
    "mlp2_residues = []\n",
    "\n",
    "for i in range(0, len(y_test)):\n",
    "    rr_residues.append(abs(y_test[i] - rr_pred[i]))\n",
    "    mlp1_residues.append(abs(y_test[i] - mlp1_pred[i]))\n",
    "    mlp2_residues.append(abs(y_test[i] - mlp2_pred[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"Ridge\": rr_residues, \"MLP1\": mlp1_residues, \"MLP2\": mlp2_residues})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) [1v] **How many iterations were required for ùëÄùêøùëÉ1 and ùëÄùêøùëÉ2 to converge?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MLP1 number of iterations:\", mlp1.n_iter_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MLP2 number of iterations:\", mlp2.n_iter_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) [1.5v] **What can be motivating the unexpected differences on the number of iterations?**\n",
    "\n",
    "**Hypothesize one reason underlying the observed performance differences between the MLPs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more about the MLP regressor at https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}

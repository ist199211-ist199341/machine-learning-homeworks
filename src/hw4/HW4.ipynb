{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework III\n",
    "\n",
    "Diogo Correia (ist199211) & Tom√°s Esteves (ist199341)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Pen and Paper [11v]\n",
    "\n",
    "**Given the bivariate observations** $\\left\\{\n",
    "\\begin{pmatrix}\n",
    "    1 \\\\\n",
    "    2\n",
    "\\end{pmatrix}\n",
    ",\n",
    "\\begin{pmatrix}\n",
    "    -1 \\\\\n",
    "    1\n",
    "\\end{pmatrix}\n",
    ",\n",
    "\\begin{pmatrix}\n",
    "    1 \\\\\n",
    "    0\n",
    "\\end{pmatrix}\n",
    "\\right\\}\n",
    "$ **and the multivariate Gaussian mixture**\n",
    "$$\n",
    "\n",
    "u_1 = \\begin{pmatrix}\n",
    "    2 \\\\\n",
    "    2\n",
    "\\end{pmatrix}\n",
    ",\n",
    "\n",
    "u_2 = \\begin{pmatrix}\n",
    "    0 \\\\\n",
    "    0\n",
    "\\end{pmatrix}\n",
    ", \n",
    "\n",
    "\\Sigma_1 = \\begin{pmatrix}\n",
    "    2 & 1 \\\\\n",
    "    1 & 2\n",
    "\\end{pmatrix}\n",
    ",\n",
    "\n",
    "\\Sigma_2 = \\begin{pmatrix}\n",
    "    2 & 0 \\\\\n",
    "    0 & 2\n",
    "\\end{pmatrix}\n",
    ",\n",
    "\n",
    "\\pi_1 = 0.5,\n",
    "\\pi_2 = 0.5\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) [7v] **Perform one epoch of the EM clustering algorithm and determine the new parameters. Indicate all calculus step by step (you can use a computer, however disclose intermediary steps).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Given the updated parameters computed in previous question:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. [1.5v] **perform a hard assignment of observations to clusters under a MAP assumption.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. [2.5v] **compute the silhouette of the larger cluster using the Euclidean distance.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming and Critical Anlaysis[9v]\n",
    "\n",
    "**Recall the pd_speech.arff dataset from earlier homeworks, centered on the Parkinson diagnosis from speech features. For the following exercises, normalize the data using sklearn‚Äôs MinMaxScaler.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) [4.5v] **Using sklearn, apply k-means clustering fully unsupervisedly (without targets) on the normalized data with ùëò = 3 and three different seeds (using random œµ {0,1,2}). Assess the silhouette and purity of the produced solutions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io.arff import loadarff\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import datasets, metrics, cluster, mixture\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loadarff(\"../data/pd_speech.arff\")\n",
    "df = pd.DataFrame(data[0])\n",
    "df[\"class\"] = df[\"class\"].str.decode(\"utf-8\")\n",
    "X = df.drop(\"class\", axis=1)\n",
    "y_true = df[\"class\"]\n",
    "\n",
    "X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = MinMaxScaler().fit_transform(X)\n",
    "\n",
    "X = pd.DataFrame(X, columns=df.columns[:-1])\n",
    "\n",
    "X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "\n",
    "\n",
    "kmeans_models = []\n",
    "\n",
    "for i in range(0, 3):\n",
    "\n",
    "    # parameterize clustering\n",
    "    kmeans_algo = cluster.KMeans(n_clusters=3, random_state=i, init=\"random\")\n",
    "\n",
    "    # learn the model\n",
    "    kmeans_model = kmeans_algo.fit(X)\n",
    "\n",
    "    # append the model to the list\n",
    "    kmeans_models.append(kmeans_model)\n",
    "\n",
    "kmeans_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 3):\n",
    "\n",
    "    y_pred = kmeans_models[i].labels_\n",
    "\n",
    "    # compute silhouette\n",
    "    print(\n",
    "        f\"{i} Silhouette (euclidean): {metrics.silhouette_score(X, y_pred, metric='euclidean')}\"\n",
    "    )\n",
    "    # print(f\"{i} Silhouette (manhattan): {metrics.silhouette_score(X, y_pred, metric='manhattan')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purity_score(y_true, y_pred):\n",
    "    # compute contingency/confusion matrix\n",
    "    confusion_matrix = metrics.cluster.contingency_matrix(y_true, y_pred)\n",
    "    return np.sum(np.amax(confusion_matrix, axis=0)) / np.sum(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 3):\n",
    "\n",
    "    y_pred = kmeans_models[i].labels_\n",
    "\n",
    "    print(\"Purity:\", purity_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) [1.5v] **What is causing the non-determinism?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">TODO\n",
    "\n",
    "Something about the random initialization of the centroids. The centroids are initialized randomly, so the results are not deterministic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) [1.5v] **Using a scatter plot, visualize side-by-side the labeled data using as labels:** \n",
    "- i) the original Parkinson diagnoses, \n",
    "- ii) the previously learned ùëò = 3 clusters (random= 0). \n",
    "\n",
    "**To this end, select the two most informative features as axes and color observations according to their label. For feature selection, select the two input variables with highest variance on the MinMax normalized data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the columns with the highest variance\n",
    "selected_features = list(X.var().sort_values(ascending=False).head(2).index)\n",
    "\n",
    "selected_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ListedColormap([\"#FF0000\", \"#0000FF\", \"#00FF00\"])\n",
    "\n",
    "# predicted data of the first model\n",
    "y_pred = kmeans_models[0].labels_\n",
    "\n",
    "# Original data\n",
    "figure = plt.figure(figsize=(14, 5))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "ax.set_facecolor(\"#BEBEBE\")\n",
    "ax.scatter(X[selected_features[0]], X[selected_features[1]], c=y_true, cmap=colors)\n",
    "plt.xlabel(selected_features[0])\n",
    "plt.ylabel(selected_features[1])\n",
    "\n",
    "# k = 3 (random = 0)\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "ax.scatter(X[selected_features[0]], X[selected_features[1]], c=y_pred, cmap=colors)\n",
    "plt.xlabel(selected_features[0])\n",
    "plt.ylabel(selected_features[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) [1.5v] **The fraction of variance explained by a principal component is the ratio between the variance of that component (i.e., its eigenvalue) and total variance (i.e., sum of all eigenvalues). How many principal components are necessary to explain more than 80% of variability? Hint: explore the DimReduction notebook to be familiar with PCA in sklearn.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.8, svd_solver=\"full\")\n",
    "\n",
    "pca.fit(X)\n",
    "\n",
    "print(pca.n_components_)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
